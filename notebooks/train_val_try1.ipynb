{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Arshad221b/MedSeg/blob/main/second_ty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4CHv0U1wPCG0"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import torch \n",
    "from torch.utils import data \n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rud7WqEHb8Ve",
    "outputId": "dd5d2c59-f066-4924-db66-f89a0b2af607"
   },
   "outputs": [],
   "source": [
    "# ! pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9Q0x1-i3cEU4"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mrdqqcEFPNP8"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7727guIDvSg"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ioZcoim2PRdO"
   },
   "outputs": [],
   "source": [
    "# ! unzip /content/drive/MyDrive/AMOS/amos22.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw0PdPCnDyZJ"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U9DyYcKammlA"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KRWGRtEvZbZK"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_7YVKIA-InO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UoQ6S3nUPGJG"
   },
   "outputs": [],
   "source": [
    "class AmosDataLoader(data.Dataset):\n",
    "  def __init__(\n",
    "      self, \n",
    "      input_paths: list, \n",
    "      target_paths: list, \n",
    "      transform_input = None, \n",
    "      transform_target = None\n",
    "  ): \n",
    "\n",
    "    self.input_paths      = input_paths\n",
    "    self.target_paths     = target_paths\n",
    "    self.transform_input  = transform_input\n",
    "    self.transform_target = transform_target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_paths)\n",
    "\n",
    "  def preprocess_img_input(self, input_im):\n",
    "    # z_factor_input      = input_im.shape[2]*int(input_im.shape[0]/IMAGE_SIZE)**2\n",
    "    input_im            = np.stack((input_im,)*3, axis=-1)\n",
    "    input_im            = torch.tensor(input_im).float()/255\n",
    "    # print(input_im.shape)\n",
    "    input_im            = input_im.permute(3,2,0,1)\n",
    "    \n",
    "    input_im            = input_im.unsqueeze(0)\n",
    "    # print('input_shape before inter',input_im.shape)\n",
    "    output_size_input   = (82, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    input_im            = F.interpolate(input_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
    "    input_im            = input_im#.squeeze(0)\n",
    "    return input_im\n",
    "\n",
    "  def preprocess_output(self, output_im):\n",
    "    mask_cat              = np.zeros((NUM_CLASS, *output_im.shape), dtype=np.float32)\n",
    "    for i in range(NUM_CLASS):\n",
    "        mask_cat[i][output_im == i] = 1\n",
    "    output_im             = torch.tensor(mask_cat).float()/255\n",
    "    output_im             = output_im.permute(0,2,3,1)\n",
    "    output_im             = output_im.unsqueeze(0)#.unsqueeze(0)\n",
    "    output_size_input     = (82, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    output_im             = F.interpolate(output_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
    "    output_im             = output_im#.squeeze(0)\n",
    "    return output_im\n",
    "\n",
    "  def __getitem__(self,x):\n",
    "    input_image   = self.input_paths[x]\n",
    "    target_image  = self.target_paths[x]\n",
    "    input_im      = nib.load(input_image).get_fdata()\n",
    "    target_im     = nib.load(target_image).get_fdata()\n",
    "\n",
    "    return input_im, target_im\n",
    "    \n",
    "  def collate_fn(self, batch):\n",
    "    # print(len(batch[0][0]))\n",
    "    im_ins, im_outs = [], []\n",
    "    for im_in, im_out  in batch: \n",
    "      im_in = self.preprocess_img_input(im_in)\n",
    "      im_out = self.preprocess_output(im_out)\n",
    "      im_ins.append(im_in)\n",
    "      im_outs.append(im_out)\n",
    "\n",
    "    # print(torch.tensor(im_ins).shape)\n",
    "    return torch.cat(im_ins, dim = 0).to(device), torch.cat(im_outs, dim= 0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hrVS5thiZtF_"
   },
   "outputs": [],
   "source": [
    "path = '/home/arshad/Downloads/amos22/amos22'\n",
    "\n",
    "\n",
    "input_paths   = sorted(glob(os.path.join(path, \"imagesTr\",\"*.nii.gz\")))\n",
    "target_paths  = sorted(glob(os.path.join(path, \"labelsTr\",\"*.nii.gz\")))\n",
    "\n",
    "val_input = sorted(glob(os.path.join(path, \"imagesVa\",\"*.nii.gz\")))\n",
    "val_target = sorted(glob(os.path.join(path, \"labelsVa\",\"*.nii.gz\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_paths = input_paths[:25]\n",
    "# target_paths = target_paths[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labelsTs',\n",
       " 'imagesTr',\n",
       " 'imagesVa',\n",
       " 'labelsVa',\n",
       " 'dataset.json',\n",
       " 'readme.md',\n",
       " '.DS_Store',\n",
       " 'imagesTs',\n",
       " 'labelsTr']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fioLA0sqaUru"
   },
   "outputs": [],
   "source": [
    "train_dl      = AmosDataLoader(input_paths, target_paths)\n",
    "train_loader  = DataLoader(train_dl, batch_size = BATCH_SIZE, drop_last= True, collate_fn=train_dl.collate_fn)\n",
    "\n",
    "val_dl      = AmosDataLoader(val_input, val_target)\n",
    "val_loader  = DataLoader(val_dl, batch_size = BATCH_SIZE, drop_last= True, collate_fn=train_dl.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGsVE4joDrAp"
   },
   "source": [
    "# Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tAXk92Ooa0YO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv3D(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mpconv(x)\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv3D(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # print(x1.shape, x2.shape)\n",
    "        x1    = self.up(x1)\n",
    "        # print(x1.shape)\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1    = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2))\n",
    "        x     = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv3D, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear     = bilinear\n",
    "\n",
    "        self.conv1    = DoubleConv3D(in_channels, 64)\n",
    "        self.down1    = Down3D(64, 128)\n",
    "        self.down2    = Down3D(128, 256)\n",
    "        self.down3    = Down3D(256, 512)\n",
    "        self.down4    = Down3D(512, 1024)\n",
    "        self.up1      = Up3D(1024, 512, bilinear)\n",
    "        self.up2      = Up3D(512, 256, bilinear)\n",
    "        self.up3      = Up3D(256, 128, bilinear)\n",
    "        self.up4      = Up3D(128, 64, bilinear)\n",
    "        self.outconv  = OutConv3D(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = x.unsqueeze(1)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        # print(x5.shape, x4.shape)\n",
    "        x6 = self.up1(x5, x4)\n",
    "        x7 = self.up2(x6, x3)\n",
    "        x8 = self.up3(x7, x2) \n",
    "        x9 = self.up4(x8, x1)\n",
    "        output= self.outconv(x9)\n",
    "        # print(x6.shape)\n",
    "        # up network\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WYdfm81hGCRD"
   },
   "outputs": [],
   "source": [
    "model = UNet3D(3, 15).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUNIpQ1h0NrO"
   },
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "avXePzyVKpha"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_batch(data, model, optimizer, criteria):\n",
    "    model.train()\n",
    "    \n",
    "    ims_in, ims_out = data\n",
    "    optimizer.zero_grad()\n",
    "    pred_img = model(ims_in)\n",
    "    total_loss = criteria(pred_img, ims_out)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def validation_batch(data, model, criteria):\n",
    "    model.eval()\n",
    "    \n",
    "    ims_in, ims_out = data\n",
    "    with torch.no_grad():\n",
    "      pred_img = model(ims_in)\n",
    "    total_loss = criteria(pred_img, ims_out)\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ekD7Nc3cSoar"
   },
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "criteria = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoSuiFsNTs76",
    "outputId": "372dea96-5e63-4b1e-9bd2-949c827373fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch)\n\u001b[1;32m      5\u001b[0m train_epoch_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, ims \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      8\u001b[0m   loss \u001b[38;5;241m=\u001b[39m train_batch(ims, model, optimizer, criteria)\n\u001b[1;32m      9\u001b[0m   train_epoch_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mAmosDataLoader.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m im_in, im_out  \u001b[38;5;129;01min\u001b[39;00m batch: \n\u001b[1;32m     56\u001b[0m   im_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_img_input(im_in)\n\u001b[0;32m---> 57\u001b[0m   im_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m   im_ins\u001b[38;5;241m.\u001b[39mappend(im_in)\n\u001b[1;32m     59\u001b[0m   im_outs\u001b[38;5;241m.\u001b[39mappend(im_out)\n",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m, in \u001b[0;36mAmosDataLoader.preprocess_output\u001b[0;34m(self, output_im)\u001b[0m\n\u001b[1;32m     33\u001b[0m mask_cat              \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((NUM_CLASS, \u001b[38;5;241m*\u001b[39moutput_im\u001b[38;5;241m.\u001b[39mshape), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_CLASS):\n\u001b[0;32m---> 35\u001b[0m     mask_cat[i][output_im \u001b[38;5;241m==\u001b[39m i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m output_im             \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mask_cat)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     37\u001b[0m output_im             \u001b[38;5;241m=\u001b[39m output_im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(n_epoch):\n",
    "  print(epoch)\n",
    "  train_epoch_loss = []\n",
    "\n",
    "  for ix, ims in enumerate(train_loader):\n",
    "    loss = train_batch(ims, model, optimizer, criteria)\n",
    "    train_epoch_loss.append(loss)\n",
    "  print('train_avg_loss', sum(train_epoch_loss)/len(train_epoch_loss))\n",
    "  train_loss.append(sum(train_epoch_loss)/len(train_epoch_loss))\n",
    "    \n",
    "  for ix, ims in enumerate(val_loader):\n",
    "    loss = validation_batch(ims, model,  criteria)\n",
    "    val_epoch_loss.append(loss)\n",
    "  print('val_avg_loss', sum(val_epoch_loss)/len(val_epoch_loss))\n",
    "  val_loss.append(sum(val_epoch_loss)/len(val_epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtnNM1vRVEKx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPauRa/mx8GFCDYnEHvCNI0",
   "collapsed_sections": [
    "B7727guIDvSg"
   ],
   "include_colab_link": true,
   "mount_file_id": "15Fdb5Qh4O08i6gsGbw8ZCiMwIXiRtFH5",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
