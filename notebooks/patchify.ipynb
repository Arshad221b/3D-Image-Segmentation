{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob \n",
    "import os \n",
    "\n",
    "from skimage import io\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "import random \n",
    "import torch \n",
    "from torch.utils import data \n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/home/arshad/Downloads/amos22/amos22'\n",
    "input_paths   = sorted(glob(os.path.join(path, \"imagesVa\",\"*.nii.gz\")))[:85]\n",
    "target_paths  = sorted(glob(os.path.join(path, \"labelsVa\",\"*.nii.gz\")))[:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASS = 15\n",
    "PATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmosDataLoader(data.Dataset):\n",
    "  def __init__(\n",
    "      self, \n",
    "      input_paths: list, \n",
    "      target_paths: list, \n",
    "      transform_input = None, \n",
    "      transform_target = None\n",
    "  ): \n",
    "\n",
    "    self.input_paths      = input_paths\n",
    "    self.target_paths     = target_paths\n",
    "    self.transform_input  = transform_input\n",
    "    self.transform_target = transform_target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_paths)\n",
    "\n",
    "  def preprocess_img_input(self, input_im):\n",
    "    img_patches = patchify(input_im, (IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE), step=IMAGE_SIZE)\n",
    "    input_img = np.reshape(img_patches, (-1, img_patches.shape[3], img_patches.shape[4], img_patches.shape[5]))\n",
    "    # input_im = np.stack((input_img,)*3, axis=-1)\n",
    "    input_im = torch.tensor(input_img).float()/255\n",
    "    input_im = input_im.unsqueeze(4)\n",
    "    input_im = input_im.permute(0,4,1,2,3)\n",
    "    return input_im\n",
    "  \n",
    "  def preprocess_img_output(self, output_im):\n",
    "    img_patches = patchify(output_im, (IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE), step=IMAGE_SIZE)\n",
    "    output_img = np.reshape(img_patches, (-1, img_patches.shape[3], img_patches.shape[4], img_patches.shape[5]))\n",
    "    output_im = np.expand_dims(output_img, axis = 4)\n",
    "    output_im = torch.tensor(output_im).float()/255\n",
    "    output_im = output_im.permute(0,4,1,2,3)\n",
    "    # output_im = output_im.squeeze(1)\n",
    "    return output_im\n",
    "\n",
    "#   def preprocess_output(self, output_im):\n",
    "#     # z_factor_output      = output_im.shape[2]*int(output_im.shape[0]/IMAGE_SIZE)**2\n",
    "#     # output_im            = torch.tensor(output_im).float()/255\n",
    "#     # print('output shape before mask', output_im.shape)\n",
    "#     mask_cat              = np.zeros((NUM_CLASS, *output_im.shape), dtype=np.float32)\n",
    "#     for i in range(NUM_CLASS):\n",
    "#         mask_cat[i][output_im == i] = 1\n",
    "#     # output_im = torch.tensor(mask_cat).float()\n",
    "#     output_im             = torch.tensor(mask_cat).float()/255\n",
    "#     # print(output_im.shape)\n",
    "#     output_im             = output_im.permute(0,2,3,1)\n",
    "#     # print(output_im.shape)\n",
    "#     output_im             = output_im.unsqueeze(0)#.unsqueeze(0)\n",
    "#     # print('output shape befor inter',output_im.shape)\n",
    "#     output_size_input     = (82, IMAGE_SIZE, IMAGE_SIZE)\n",
    "#     output_im             = F.interpolate(output_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
    "#     output_im             = output_im#.squeeze(0)\n",
    "    \n",
    "    \n",
    "#     # print('output shape final', output_im.shape)\n",
    "#     return output_im\n",
    "\n",
    "  def __getitem__(self,x):\n",
    "    input_image = self.input_paths[x]\n",
    "    mask_image  = self.target_paths[x]\n",
    "    input_im    = nib.load(input_image).get_fdata()\n",
    "    mask_im     = nib.load(mask_image).get_fdata()\n",
    "\n",
    "    return input_im, mask_im\n",
    "    \n",
    "  def collate_fn(self, batch):\n",
    "    # print(len(batch[0][0]))\n",
    "    im_ins, im_outs = [], []\n",
    "    for im_in, im_out  in batch: \n",
    "      im_in = self.preprocess_img_input(im_in)\n",
    "      im_out = self.preprocess_img_output(im_out)\n",
    "\n",
    "      # im_out = self.preprocess_output(im_out)\n",
    "      # print(im_in.shape, im_out.shape)\n",
    "      im_ins.append(im_in)\n",
    "      im_outs.append(im_out)\n",
    "\n",
    "    # print(torch.tensor(im_ins).shape)\n",
    "    return torch.cat(im_ins, dim = 0), torch.cat(im_outs, dim= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl      = AmosDataLoader(input_paths, target_paths)\n",
    "train_loader  = DataLoader(train_dl, batch_size = BATCH_SIZE, drop_last= True, collate_fn=train_dl.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d,k in train_dl: \n",
    "#   # # print(len(d))}\n",
    "#   print(d.shape, k.shape)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d, k in train_loader:\n",
    "#     print(d.shape, k.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv3D(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mpconv(x)\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv3D(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # print(x1.shape, x2.shape)\n",
    "        x1    = self.up(x1)\n",
    "        # print(x1.shape)\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1    = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2))\n",
    "        x     = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv3D, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear     = bilinear\n",
    "\n",
    "        self.conv1    = DoubleConv3D(in_channels, 64)\n",
    "        self.down1    = Down3D(64, 128)\n",
    "        self.down2    = Down3D(128, 256)\n",
    "        self.down3    = Down3D(256, 512)\n",
    "        self.down4    = Down3D(512, 1024)\n",
    "        self.up1      = Up3D(1024, 512, bilinear)\n",
    "        self.up2      = Up3D(512, 256, bilinear)\n",
    "        self.up3      = Up3D(256, 128, bilinear)\n",
    "        self.up4      = Up3D(128, 64, bilinear)\n",
    "        self.outconv  = OutConv3D(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = x.unsqueeze(1)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        # print(x5.shape, x4.shape)\n",
    "        x6 = self.up1(x5, x4)\n",
    "        x7 = self.up2(x6, x3)\n",
    "        x8 = self.up3(x7, x2) \n",
    "        x9 = self.up4(x8, x1)\n",
    "        output= self.outconv(x9)\n",
    "        # print(x6.shape)\n",
    "        # up network\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3D(1, NUM_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input_im, target):\n",
    "    smooth = 1.0\n",
    "    \n",
    "\n",
    "    iflat = input_im#.flatten()\n",
    "    tflat = target#.flatten()\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(data, model, optimizer, criteria):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    ims_in, ims_out = data\n",
    "    # print(ims_in.shape)\n",
    "    optimizer.zero_grad()\n",
    "    # pred_img = torch.tensor(ims_out.shape)\n",
    "    # print(ims_in.shape)\n",
    "    for k in range(0, len(ims_in), PATCH_SIZE):\n",
    "        ims_in_temp = ims_in[k:k+PATCH_SIZE]\n",
    "        ims_out_temp = ims_out[k:k+PATCH_SIZE]\n",
    "        pred_img_temp = model(ims_in_temp.to(device))  \n",
    "        # pred_img_temp = F.softmax(pred_img_temp, 1)\n",
    "        # pred_img_temp = pred_img_temp.argmax(dim=1)\n",
    "        softmax_output = F.softmax(pred_img_temp, dim=1)\n",
    "        index_tensor = torch.arange(0, 15, dtype=torch.float, device=device)\n",
    "        index_tensor = index_tensor.view(1, -1, 1, 1,1).expand_as(pred_img_temp)\n",
    "        pred_img_temp = torch.sum(index_tensor * softmax_output, dim=1, keepdim=True)\n",
    "        # pred_img_temp = torch.argmax(pred_img_temp, 1)\n",
    "        \n",
    "        # pred_img_temp = pred_img_temp.contiguous().view(-1)\n",
    "        # ims_out_temp = ims_out_temp.contiguous().view(-1)\n",
    "        # batch, dims, height, width, depth = pred_img_temp.shape\n",
    "        \n",
    "        # print(type(pred_img_temp))\n",
    "        # _, pred_img_temp = torch.max(pred_img_temp, dim=1)\n",
    "        # print(pred_img_temp.shape, ims_out_temp.shape)\n",
    "        optimizer.zero_grad()\n",
    "        # loss = dice_loss(pred_img_temp.to('cpu'), ims_out_temp)\n",
    "        loss = criteria(pred_img_temp.to('cpu'), ims_out_temp)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss)\n",
    "    # print(pred_img.shape)\n",
    "    # gender_criterion, age_criterion = criteria\n",
    "    # gender_loss = gender_criterion(pred_gender.squeeze(), gender)\n",
    "    # age_loss = age_criterion(pred_age.squeeze(), age)\n",
    "        \n",
    "    total_loss = sum(total_loss)/len(total_loss)\n",
    "    \n",
    "    # total_loss.backward()\n",
    "    # optimizer.step()\n",
    "    return total_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "criteria = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, model):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    savepath=\"checkpoint_{}.t7\".format(str(epoch))\n",
    "    torch.save(state,savepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "avg_loss tensor(8.7517, grad_fn=<DivBackward0>)\n",
      "1\n",
      "avg_loss tensor(0.4533, grad_fn=<DivBackward0>)\n",
      "2\n",
      "avg_loss tensor(0.3255, grad_fn=<DivBackward0>)\n",
      "3\n",
      "avg_loss tensor(0.3103, grad_fn=<DivBackward0>)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "for epoch in range(n_epoch):\n",
    "  print(epoch)\n",
    "  epoch_loss = []\n",
    "\n",
    "  for ix, ims in enumerate(train_loader):\n",
    "    loss, model = train_batch(ims, model, optimizer, criteria)\n",
    "    epoch_loss.append(loss)\n",
    "    \n",
    "  if epoch % 2 == 0:\n",
    "    save_model(epoch, model)\n",
    "  print('avg_loss', sum(epoch_loss)/len(epoch_loss))\n",
    "  train_loss.append(sum(epoch_loss)/len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
