{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B7727guIDvSg"
      ],
      "mount_file_id": "15Fdb5Qh4O08i6gsGbw8ZCiMwIXiRtFH5",
      "authorship_tag": "ABX9TyPauRa/mx8GFCDYnEHvCNI0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arshad221b/MedSeg/blob/main/second_ty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4CHv0U1wPCG0"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import torch \n",
        "from torch.utils import data \n",
        "import torchvision.transforms.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nibabel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rud7WqEHb8Ve",
        "outputId": "dd5d2c59-f066-4924-db66-f89a0b2af607"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.9/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.9/dist-packages (from nibabel) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib"
      ],
      "metadata": {
        "id": "9Q0x1-i3cEU4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "mrdqqcEFPNP8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "B7727guIDvSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! unzip /content/drive/MyDrive/AMOS/amos22.zip"
      ],
      "metadata": {
        "id": "ioZcoim2PRdO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "Sw0PdPCnDyZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 2\n",
        "NUM_CLASS = 15"
      ],
      "metadata": {
        "id": "U9DyYcKammlA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "KRWGRtEvZbZK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_7YVKIA-InO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AmosDataLoader(data.Dataset):\n",
        "  def __init__(\n",
        "      self, \n",
        "      input_paths: list, \n",
        "      target_paths: list, \n",
        "      transform_input = None, \n",
        "      transform_target = None\n",
        "  ): \n",
        "\n",
        "    self.input_paths      = input_paths\n",
        "    self.target_paths     = target_paths\n",
        "    self.transform_input  = transform_input\n",
        "    self.transform_target = transform_target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_paths)\n",
        "\n",
        "  def preprocess_img_input(self, input_im):\n",
        "    # z_factor_input      = input_im.shape[2]*int(input_im.shape[0]/IMAGE_SIZE)**2\n",
        "    input_im            = np.stack((input_im,)*3, axis=-1)\n",
        "    input_im            = torch.tensor(input_im).float()/255\n",
        "    # print(input_im.shape)\n",
        "    input_im            = input_im.permute(3,2,0,1)\n",
        "    \n",
        "    input_im            = input_im.unsqueeze(0)\n",
        "    # print('input_shape before inter',input_im.shape)\n",
        "    output_size_input   = (82, IMAGE_SIZE, IMAGE_SIZE)\n",
        "    input_im            = F.interpolate(input_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
        "    input_im            = input_im#.squeeze(0)\n",
        "    return input_im\n",
        "\n",
        "  def preprocess_output(self, output_im):\n",
        "    # z_factor_output      = output_im.shape[2]*int(output_im.shape[0]/IMAGE_SIZE)**2\n",
        "    # output_im            = torch.tensor(output_im).float()/255\n",
        "    # print('output shape before mask', output_im.shape)\n",
        "    mask_cat              = np.zeros((NUM_CLASS, *output_im.shape), dtype=np.float32)\n",
        "    for i in range(NUM_CLASS):\n",
        "        mask_cat[i][output_im == i] = 1\n",
        "    # output_im = torch.tensor(mask_cat).float()\n",
        "    output_im             = torch.tensor(mask_cat).float()/255\n",
        "    # print(output_im.shape)\n",
        "    output_im             = output_im.permute(0,2,3,1)\n",
        "    # print(output_im.shape)\n",
        "    output_im             = output_im.unsqueeze(0)#.unsqueeze(0)\n",
        "    # print('output shape befor inter',output_im.shape)\n",
        "    output_size_input     = (82, IMAGE_SIZE, IMAGE_SIZE)\n",
        "    output_im             = F.interpolate(output_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
        "    output_im             = output_im#.squeeze(0)\n",
        "    \n",
        "    \n",
        "    # print('output shape final', output_im.shape)\n",
        "    return output_im\n",
        "\n",
        "  def __getitem__(self,x):\n",
        "    input_image   = self.input_paths[x]\n",
        "    target_image  = self.target_paths[x]\n",
        "    input_im      = nib.load(input_image).get_fdata()\n",
        "    target_im     = nib.load(target_image).get_fdata()\n",
        "\n",
        "    return input_im, target_im\n",
        "    \n",
        "  def collate_fn(self, batch):\n",
        "    # print(len(batch[0][0]))\n",
        "    im_ins, im_outs = [], []\n",
        "    for im_in, im_out  in batch: \n",
        "      im_in = self.preprocess_img_input(im_in)\n",
        "      im_out = self.preprocess_output(im_out)\n",
        "      # print(im_in.shape, im_out.shape)\n",
        "      im_ins.append(im_in)\n",
        "      im_outs.append(im_out)\n",
        "\n",
        "    # print(torch.tensor(im_ins).shape)\n",
        "    return torch.cat(im_ins, dim = 0).to(device), torch.cat(im_outs, dim= 0).to(device)"
      ],
      "metadata": {
        "id": "UoQ6S3nUPGJG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/amos22/'\n",
        "\n",
        "\n",
        "input_paths   = sorted(glob(os.path.join(path, \"imagesVa\",\"*.nii.gz\")))\n",
        "target_paths  = sorted(glob(os.path.join(path, \"labelsVa\",\"*.nii.gz\")))"
      ],
      "metadata": {
        "id": "hrVS5thiZtF_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl      = AmosDataLoader(input_paths, target_paths)\n",
        "train_loader  = DataLoader(train_dl, batch_size = BATCH_SIZE, drop_last= True, collate_fn=train_dl.collate_fn)"
      ],
      "metadata": {
        "id": "fioLA0sqaUru"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d,k in train_dl: \n",
        "  # # print(len(d))}\n",
        "  print(d.shape, k.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "M1Lu0kq5b7b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb203998-7787-4815-d8f0-e8199b5cc2ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768, 768, 101) (768, 768, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_output( output_im):\n",
        "#     z_factor_output      = output_im.shape[2]*int(output_im.shape[0]/IMAGE_SIZE)**2\n",
        "#     # output_im            = torch.tensor(output_im).float()/255\n",
        "#     mask_cat              = np.zeros((NUM_CLASS, *output_im.shape), dtype=np.float32)\n",
        "#     for i in range(NUM_CLASS):\n",
        "#         mask_cat[i][output_im == i] = 1\n",
        "\n",
        "#     output_im             = torch.tensor(mask_cat).float()/255\n",
        "#     output_im             = output_im.permute(3,2,0,1)\n",
        "#     # print(output_im.shape)\n",
        "#     output_im             = output_im.unsqueeze(0)\n",
        "#     output_size_input     = (15, IMAGE_SIZE, IMAGE_SIZE)\n",
        "#     output_im             = F.interpolate(output_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
        "#     output_im             = output_im#.squeeze(0)\n",
        "#     return output_im"
      ],
      "metadata": {
        "id": "XtIHG6Xq20_2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchvision.transforms.functional as TF \n",
        "# target_image = target_paths[0]\n",
        "# mask_nii     = nib.load(target_image)\n",
        "# mask_data = mask_nii.get_fdata()\n",
        "\n",
        "# o = preprocess_output(mask_data)\n",
        "# # target_im  = torch.tensor(target_im)\n",
        "\n",
        "# num_classes = 15\n",
        "\n",
        "# # Convert the mask to categorical values\n",
        "# mask_cat = np.zeros((num_classes, *mask_data.shape), dtype=np.float32)\n",
        "# for i in range(num_classes):\n",
        "#     mask_cat[i][mask_data == i] = 1\n",
        "\n",
        "# # Save the categorical mask as a new NIfTI image\n",
        "# mask_cat_nii = nib.Nifti1Image(mask_cat, mask_nii.affine, mask_nii.header)\n",
        "# # nib.save(mask_cat_nii, \"mask_cat.nii.gz\")"
      ],
      "metadata": {
        "id": "W5kKlwtntCQ0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# o.shape"
      ],
      "metadata": {
        "id": "OM2hpCtO3Ane"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o = 0\n",
        "mask_data = 0\n",
        "mask_nii = 0 \n"
      ],
      "metadata": {
        "id": "THpaDPJT3Wmd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask_cat.shape"
      ],
      "metadata": {
        "id": "VRpiVvhQvz88"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "\n",
        "test_load = nib.load('/content/amos22/labelsVa/amos_0008.nii.gz').get_fdata()\n",
        "test = test_load[:,:,21]\n",
        "plt.imshow(test)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "N1fd1WQmup8Y",
        "outputId": "2ad191c2-87fa-4213-e319-f39ba8e1fa26"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, j in train_dl: \n",
        "#   print(i.shape, j.shape)\n",
        "#   i = i.permute(2,1,0)\n",
        "#   plt.imshow(i[:,:,59].unsqeeze(0).to('cpu'))\n",
        "#   plt.show()N\n",
        "#   # print(i)\n",
        "#   break"
      ],
      "metadata": {
        "id": "4nAwMVJXbPjg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# # Assume tensor of shape (batch_size, channels, depth, height, width)\n",
        "# x = torch.randn(1, 1, 90, 768, 768)\n",
        "\n",
        "# # Specify the output size of the tensor after interpolation\n",
        "# output_size = (810,  256, 256)\n",
        "\n",
        "# # Interpolate the tensor to the desired output size using trilinear interpolation\n",
        "# y = F.interpolate(x, size=output_size, mode='trilinear', align_corners=False)\n",
        "\n",
        "# # The shape of the resulting tensor should be (batch_size, channels, depth, height, width)\n",
        "# print(y.shape)"
      ],
      "metadata": {
        "id": "9hskYVprmMAw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iH43xw0BOoQe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a,b = next(iter(train_loader))\n",
        "# a.shape, b.shape"
      ],
      "metadata": {
        "id": "tZ_azzwzaifB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPB3L7WsaoVa",
        "outputId": "f20aff99-d1f8-4b9d-bb54-ef5d1cdf5d1f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/amos22/imagesVa/amos_0008.nii.gz',\n",
              " '/content/amos22/imagesVa/amos_0013.nii.gz',\n",
              " '/content/amos22/imagesVa/amos_0018.nii.gz',\n",
              " '/content/amos22/imagesVa/amos_0022.nii.gz',\n",
              " '/content/amos22/imagesVa/amos_0029.nii.gz',\n",
              " '/content/amos22/imagesVa/amos_0032.nii.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building\n"
      ],
      "metadata": {
        "id": "vGsVE4joDrAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Down3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool3d(2),\n",
        "            DoubleConv3D(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mpconv(x)\n",
        "\n",
        "class Up3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose3d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv = DoubleConv3D(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # print(x1.shape, x2.shape)\n",
        "        x1    = self.up(x1)\n",
        "        # print(x1.shape)\n",
        "        diffZ = x2.size()[2] - x1.size()[2]\n",
        "        diffY = x2.size()[3] - x1.size()[3]\n",
        "        diffX = x2.size()[4] - x1.size()[4]\n",
        "        x1    = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2,\n",
        "                        diffZ // 2, diffZ - diffZ // 2))\n",
        "        x     = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv3D, self).__init__()\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
        "        super().__init__()\n",
        "        self.in_channels  = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.bilinear     = bilinear\n",
        "\n",
        "        self.conv1    = DoubleConv3D(in_channels, 64)\n",
        "        self.down1    = Down3D(64, 128)\n",
        "        self.down2    = Down3D(128, 256)\n",
        "        self.down3    = Down3D(256, 512)\n",
        "        self.down4    = Down3D(512, 1024)\n",
        "        self.up1      = Up3D(1024, 512, bilinear)\n",
        "        self.up2      = Up3D(512, 256, bilinear)\n",
        "        self.up3      = Up3D(256, 128, bilinear)\n",
        "        self.up4      = Up3D(128, 64, bilinear)\n",
        "        self.outconv  = OutConv3D(64, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        # x = x.unsqueeze(1)\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        # print(x5.shape, x4.shape)\n",
        "        x6 = self.up1(x5, x4)\n",
        "        x7 = self.up2(x6, x3)\n",
        "        x8 = self.up3(x7, x2) \n",
        "        x9 = self.up4(x8, x1)\n",
        "        output= self.outconv(x9)\n",
        "        # print(x6.shape)\n",
        "        # up network\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "tAXk92Ooa0YO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet3D(3, 15).to(device)"
      ],
      "metadata": {
        "id": "WYdfm81hGCRD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Train"
      ],
      "metadata": {
        "id": "DUNIpQ1h0NrO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oX4l1JVM0NQh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# im= torch.zeros((2, 3,82,128,128))\n",
        "# UNet3D(3,15).forward(im)"
      ],
      "metadata": {
        "id": "itTQ9SMSGHqd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install torchsummary\n",
        "# from torchsummary import summary\n",
        "# summary(model, input_size=(3, 82, 128, 128), device=device)"
      ],
      "metadata": {
        "id": "IDGOJGbSGT3f"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(data, model, optimizer, criteria):\n",
        "    model.train()\n",
        "    \n",
        "    ims_in, ims_out = data\n",
        "    # print(ims_in.shape)\n",
        "    optimizer.zero_grad()\n",
        "    # print(ims_in.shape)\n",
        "    pred_img = model(ims_in)    \n",
        "    print(pred_img.shape)\n",
        "    # gender_criterion, age_criterion = criteria\n",
        "    # gender_loss = gender_criterion(pred_gender.squeeze(), gender)\n",
        "    # age_loss = age_criterion(pred_age.squeeze(), age)\n",
        "    total_loss = criteria(pred_img, ims_out)\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    return total_loss\n",
        "\n",
        "# def validate_batch(data, model, criteria):\n",
        "#     model.eval()\n",
        "#     ims, age, gender = data\n",
        "#     with torch.no_grad():\n",
        "#       pred_gender, pred_age = model(ims)\n",
        "#     gender_criterion, age_criterion = criteria\n",
        "#     gender_loss = gender_criterion(pred_gender.squeeze(), gender)\n",
        "#     age_loss = age_criterion(pred_age.squeeze(), age)\n",
        "#     total_loss = gender_loss + age_loss\n",
        "#     pred_gender = (pred_gender > 0.5).squeeze()\n",
        "#     gender_acc = (pred_gender == gender).float().sum()\n",
        "#     age_mae = torch.abs(age - pred_age).float().sum()\n",
        "#     return total_loss, gender_acc, age_mae"
      ],
      "metadata": {
        "id": "avXePzyVKpha"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 2\n",
        "criteria = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "ekD7Nc3cSoar"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epoch):\n",
        "  print(epoch)\n",
        "  epoch_loss = []\n",
        "\n",
        "  for ix, ims in enumerate(train_loader):\n",
        "    loss = train_batch(ims, model, optimizer, criteria)\n",
        "    epoch_loss.append(loss)\n",
        "  print('avg_loss', sum(epoch_loss)/len(epoch_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoSuiFsNTs76",
        "outputId": "372dea96-5e63-4b1e-9bd2-949c827373fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "torch.Size([2, 15, 82, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xtnNM1vRVEKx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}