{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob \n",
    "import os \n",
    "\n",
    "from skimage import io\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "import random \n",
    "import torch \n",
    "from torch.utils import data \n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/Users/arshad_221b/Downloads/Projects/MedSeg/amos22 2'\n",
    "input_paths   = sorted(glob(os.path.join(path, \"imagesVa\",\"*.nii.gz\")))[:2]\n",
    "target_paths  = sorted(glob(os.path.join(path, \"labelsVa\",\"*.nii.gz\")))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/arshad_221b/Downloads/Projects/MedSeg/amos22 2/imagesVa/amos_0008.nii.gz',\n",
       " '/Users/arshad_221b/Downloads/Projects/MedSeg/amos22 2/imagesVa/amos_0013.nii.gz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 2\n",
    "NUM_CLASS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmosDataLoader(data.Dataset):\n",
    "  def __init__(\n",
    "      self, \n",
    "      input_paths: list, \n",
    "      target_paths: list, \n",
    "      transform_input = None, \n",
    "      transform_target = None\n",
    "  ): \n",
    "\n",
    "    self.input_paths      = input_paths\n",
    "    self.target_paths     = target_paths\n",
    "    self.transform_input  = transform_input\n",
    "    self.transform_target = transform_target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_paths)\n",
    "\n",
    "  def preprocess_img_input(self, input_im):\n",
    "    img_patches = patchify(input_im, (IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE), step=IMAGE_SIZE)\n",
    "    input_img = np.reshape(img_patches, (-1, img_patches.shape[3], img_patches.shape[4], img_patches.shape[5]))\n",
    "    input_im = np.stack((input_img,)*3, axis=-1)\n",
    "    input_im = torch.tensor(input_im).float()/255\n",
    "    \n",
    "    input_im = input_im.permute(0,4,1,2,3)\n",
    "    return input_im\n",
    "  \n",
    "  def preprocess_img_output(self, output_im):\n",
    "    img_patches = patchify(output_im, (IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE), step=IMAGE_SIZE)\n",
    "    output_img = np.reshape(img_patches, (-1, img_patches.shape[3], img_patches.shape[4], img_patches.shape[5]))\n",
    "    output_im = np.expand_dims(output_img, axis = 4)\n",
    "    output_im = torch.tensor(output_im).float()/255\n",
    "    \n",
    "    output_im = output_im.permute(0,4,1,2,3)\n",
    "    return output_im\n",
    "\n",
    "#   def preprocess_output(self, output_im):\n",
    "#     # z_factor_output      = output_im.shape[2]*int(output_im.shape[0]/IMAGE_SIZE)**2\n",
    "#     # output_im            = torch.tensor(output_im).float()/255\n",
    "#     # print('output shape before mask', output_im.shape)\n",
    "#     mask_cat              = np.zeros((NUM_CLASS, *output_im.shape), dtype=np.float32)\n",
    "#     for i in range(NUM_CLASS):\n",
    "#         mask_cat[i][output_im == i] = 1\n",
    "#     # output_im = torch.tensor(mask_cat).float()\n",
    "#     output_im             = torch.tensor(mask_cat).float()/255\n",
    "#     # print(output_im.shape)\n",
    "#     output_im             = output_im.permute(0,2,3,1)\n",
    "#     # print(output_im.shape)\n",
    "#     output_im             = output_im.unsqueeze(0)#.unsqueeze(0)\n",
    "#     # print('output shape befor inter',output_im.shape)\n",
    "#     output_size_input     = (82, IMAGE_SIZE, IMAGE_SIZE)\n",
    "#     output_im             = F.interpolate(output_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
    "#     output_im             = output_im#.squeeze(0)\n",
    "    \n",
    "    \n",
    "#     # print('output shape final', output_im.shape)\n",
    "#     return output_im\n",
    "\n",
    "  def __getitem__(self,x):\n",
    "    input_image = self.input_paths[x]\n",
    "    mask_image  = self.target_paths[x]\n",
    "    input_im    = nib.load(input_image).get_fdata()\n",
    "    mask_im     = nib.load(mask_image).get_fdata()\n",
    "\n",
    "    return input_im, mask_im\n",
    "    \n",
    "  def collate_fn(self, batch):\n",
    "    # print(len(batch[0][0]))\n",
    "    im_ins, im_outs = [], []\n",
    "    for im_in, im_out  in batch: \n",
    "      im_in = self.preprocess_img_input(im_in)\n",
    "      im_out = self.preprocess_img_out(im_out)\n",
    "\n",
    "      # im_out = self.preprocess_output(im_out)\n",
    "      # print(im_in.shape, im_out.shape)\n",
    "      im_ins.append(im_in)\n",
    "      im_outs.append(im_out)\n",
    "\n",
    "    # print(torch.tensor(im_ins).shape)\n",
    "    return torch.cat(im_ins, dim = 0).to(device), torch.cat(im_outs, dim= 0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl      = AmosDataLoader(input_paths, target_paths)\n",
    "train_loader  = DataLoader(train_dl, batch_size = BATCH_SIZE, drop_last= True, collate_fn=train_dl.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 768, 101) (768, 768, 101)\n"
     ]
    }
   ],
   "source": [
    "for d,k in train_dl: \n",
    "  # # print(len(d))}\n",
    "  print(d.shape, k.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 3, 64, 64, 64]) torch.Size([208, 3, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for d, k in train_loader:\n",
    "    print(d.shape, k.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv3D(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mpconv(x)\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv3D(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # print(x1.shape, x2.shape)\n",
    "        x1    = self.up(x1)\n",
    "        # print(x1.shape)\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1    = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2))\n",
    "        x     = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv3D, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear     = bilinear\n",
    "\n",
    "        self.conv1    = DoubleConv3D(in_channels, 64)\n",
    "        self.down1    = Down3D(64, 128)\n",
    "        self.down2    = Down3D(128, 256)\n",
    "        self.down3    = Down3D(256, 512)\n",
    "        self.down4    = Down3D(512, 1024)\n",
    "        self.up1      = Up3D(1024, 512, bilinear)\n",
    "        self.up2      = Up3D(512, 256, bilinear)\n",
    "        self.up3      = Up3D(256, 128, bilinear)\n",
    "        self.up4      = Up3D(128, 64, bilinear)\n",
    "        self.outconv  = OutConv3D(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = x.unsqueeze(1)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        # print(x5.shape, x4.shape)\n",
    "        x6 = self.up1(x5, x4)\n",
    "        x7 = self.up2(x6, x3)\n",
    "        x8 = self.up3(x7, x2) \n",
    "        x9 = self.up4(x8, x1)\n",
    "        output= self.outconv(x9)\n",
    "        # print(x6.shape)\n",
    "        # up network\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3D(3, 15).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(data, model, optimizer, criteria):\n",
    "    model.train()\n",
    "    \n",
    "    ims_in, ims_out = data\n",
    "    # print(ims_in.shape)\n",
    "    optimizer.zero_grad()\n",
    "    # print(ims_in.shape)\n",
    "    pred_img = model(ims_in)    \n",
    "    # print(pred_img.shape)\n",
    "    # gender_criterion, age_criterion = criteria\n",
    "    # gender_loss = gender_criterion(pred_gender.squeeze(), gender)\n",
    "    # age_loss = age_criterion(pred_age.squeeze(), age)\n",
    "    total_loss = criteria(pred_img, ims_out)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 1\n",
    "criteria = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m epoch_loss \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m ix, ims \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m----> 7\u001b[0m   loss \u001b[39m=\u001b[39m train_batch(ims, model, optimizer, criteria)\n\u001b[1;32m      8\u001b[0m   epoch_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mavg_loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39msum\u001b[39m(epoch_loss)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(epoch_loss))\n",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(data, model, optimizer, criteria)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m \u001b[39m# print(ims_in.shape)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pred_img \u001b[39m=\u001b[39m model(ims_in)    \n\u001b[1;32m      9\u001b[0m \u001b[39m# print(pred_img.shape)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# gender_criterion, age_criterion = criteria\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# gender_loss = gender_criterion(pred_gender.squeeze(), gender)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# age_loss = age_criterion(pred_age.squeeze(), age)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m total_loss \u001b[39m=\u001b[39m criteria(pred_img, ims_out)\n",
      "File \u001b[0;32m~/Downloads/Projects/MedSeg/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 84\u001b[0m, in \u001b[0;36mUNet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     82\u001b[0m     \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[39m# x = x.unsqueeze(1)\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     85\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown1(x1)\n\u001b[1;32m     86\u001b[0m     x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown2(x2)\n",
      "File \u001b[0;32m~/Downloads/Projects/MedSeg/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mDoubleConv3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n",
      "File \u001b[0;32m~/Downloads/Projects/MedSeg/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/Projects/MedSeg/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/Projects/MedSeg/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/Projects/MedSeg/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Downloads/Projects/MedSeg/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\n\u001b[1;32m    609\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[1;32m    610\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "for epoch in range(n_epoch):\n",
    "  print(epoch)\n",
    "  epoch_loss = []\n",
    "\n",
    "  for ix, ims in enumerate(train_loader):\n",
    "    loss = train_batch(ims, model, optimizer, criteria)\n",
    "    epoch_loss.append(loss)\n",
    "  print('avg_loss', sum(epoch_loss)/len(epoch_loss))\n",
    "  train_loss.append(sum(epoch_loss)/len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
