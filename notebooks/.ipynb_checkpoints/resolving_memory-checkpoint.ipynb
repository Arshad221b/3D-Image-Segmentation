{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob \n",
    "import os \n",
    "\n",
    "from skimage import io\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "import random \n",
    "import torch \n",
    "from torch.utils import data \n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/home/arshad/Downloads/amos22/amos22'\n",
    "input_paths   = sorted(glob(os.path.join(path, \"imagesVa\",\"*.nii.gz\")))[:25]\n",
    "target_paths  = sorted(glob(os.path.join(path, \"labelsVa\",\"*.nii.gz\")))[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ids = [0, 1] # GPUs 0 and 1\n",
    "devices = [torch.device(f'cuda:{i}') for i in device_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[device(type='cuda', index=0), device(type='cuda', index=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmosDataLoader(data.Dataset):\n",
    "  def __init__(\n",
    "      self, \n",
    "      input_paths: list, \n",
    "      target_paths: list, \n",
    "      transform_input = None, \n",
    "      transform_target = None\n",
    "  ): \n",
    "\n",
    "    self.input_paths      = input_paths\n",
    "    self.target_paths     = target_paths\n",
    "    self.transform_input  = transform_input\n",
    "    self.transform_target = transform_target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_paths)\n",
    "\n",
    "  def preprocess_img_input(self, input_im):\n",
    "    input_im = patchify(input_im, (IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE), step=IMAGE_SIZE)\n",
    "    input_im = np.reshape(input_im, (-1, input_im.shape[3], input_im.shape[4], input_im.shape[5]))\n",
    "    input_im = np.stack((input_im,)*3, axis=-1) #reduced to the 1 channel\n",
    "    input_im = torch.tensor(input_im).float()/255\n",
    "    \n",
    "    input_im = input_im.permute(0,4,1,2,3)\n",
    "    return input_im\n",
    "  \n",
    "  def preprocess_img_output(self, output_im):\n",
    "    output_im = patchify(output_im, (IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE), step=IMAGE_SIZE)\n",
    "    output_im = np.reshape(output_im, (-1, output_im.shape[3], output_im.shape[4], output_im.shape[5]))\n",
    "    output_im = np.expand_dims(output_im, axis = 4)\n",
    "    output_im = torch.tensor(output_im).float()/255\n",
    "    \n",
    "    output_im = output_im.permute(0,4,1,2,3)\n",
    "    return output_im\n",
    "\n",
    "\n",
    "  def __getitem__(self,x):\n",
    "    input_im = self.input_paths[x]\n",
    "    mask_im  = self.target_paths[x]\n",
    "    input_im    = nib.load(input_im).get_fdata()\n",
    "    mask_im     = nib.load(mask_im).get_fdata()\n",
    "\n",
    "    return input_im, mask_im\n",
    "    \n",
    "  def collate_fn(self, batch):\n",
    "    # print(len(batch[0][0]))\n",
    "    im_ins, im_outs = [], []\n",
    "    for im_in, im_out  in batch: \n",
    "      im_in = self.preprocess_img_input(im_in)\n",
    "      im_out = self.preprocess_img_output(im_out)\n",
    "\n",
    "      # im_out = self.preprocess_output(im_out)\n",
    "      # print(im_in.shape, im_out.shape)\n",
    "      im_ins.append(im_in)\n",
    "      im_outs.append(im_out)\n",
    "\n",
    "    # print(torch.tensor(im_ins).shape)\n",
    "    return torch.cat(im_ins, dim = 0), torch.cat(im_outs, dim= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl      = AmosDataLoader(input_paths, target_paths)\n",
    "train_loader  = DataLoader(train_dl, batch_size = BATCH_SIZE, drop_last= True, collate_fn=train_dl.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d,k in train_dl: \n",
    "#   # # print(len(d))}\n",
    "#   print(d.shape, k.shape)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d, k in train_loader:\n",
    "#     print(d.shape, k.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv3D(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mpconv(x)\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv3D(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # print(x1.shape, x2.shape)\n",
    "        x1    = self.up(x1)\n",
    "        # print(x1.shape)\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1    = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2))\n",
    "        x     = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv3D, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear     = bilinear\n",
    "\n",
    "        self.conv1    = DoubleConv3D(in_channels, 64)\n",
    "        self.down1    = Down3D(64, 128)\n",
    "        self.down2    = Down3D(128, 256)\n",
    "        self.down3    = Down3D(256, 512)\n",
    "        self.down4    = Down3D(512, 1024)\n",
    "        self.up1      = Up3D(1024, 512, bilinear)\n",
    "        self.up2      = Up3D(512, 256, bilinear)\n",
    "        self.up3      = Up3D(256, 128, bilinear)\n",
    "        self.up4      = Up3D(128, 64, bilinear)\n",
    "        self.outconv  = OutConv3D(64, out_channels)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # print(x.shape)\n",
    "    #     # x = x.unsqueeze(1)\n",
    "    #     x1 = self.conv1(x)\n",
    "    #     x2 = self.down1(x1)\n",
    "    #     x3 = self.down2(x2)\n",
    "    #     x4 = self.down3(x3)\n",
    "    #     x5 = self.down4(x4)\n",
    "    #     # print(x5.shape, x4.shape)\n",
    "    #     x6 = self.up1(x5, x4)\n",
    "    #     x7 = self.up2(x6, x3)\n",
    "    #     x8 = self.up3(x7, x2) \n",
    "    #     x9 = self.up4(x8, x1)\n",
    "    #     output= self.outconv(x9)\n",
    "    #     # print(x6.shape)\n",
    "    #     # up network\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = x.unsqueeze(1)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        # x4 = self.down3(x3)\n",
    "        # x5 = self.down4(x4)\n",
    "        # print(x5.shape, x4.shape)\n",
    "        # x = self.up1(x5, x4)\n",
    "        # x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2) \n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        output= self.outconv(x)\n",
    "        # print(x6.shape)\n",
    "        # up network\n",
    "        del x1\n",
    "        del x2\n",
    "        del x3\n",
    "        del x4\n",
    "        del x5\n",
    "        del x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(UNet3D(3, NUM_CLASS)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss(A, B, reduction='none'): \n",
    "    out = A.clone()\n",
    "    out -= B\n",
    "    return out.pow_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(data, model, optimizer):\n",
    "    model.train()\n",
    "    ims_in, ims_out = data\n",
    "    optimizer.zero_grad()\n",
    "    pred_img = model(ims_in.to(device))    \n",
    "    total_loss = myloss(pred_img.to('cpu'), ims_out) #custom loss function\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 1\n",
    "#criteria = #nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_29007/3434515746.py\", line 102, in forward\n    x2 = self.down1(x1)\n         ^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_29007/3434515746.py\", line 29, in forward\n    return self.mpconv(x)\n           ^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_29007/3434515746.py\", line 18, in forward\n    return self.conv(x)\n           ^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n    return F.batch_norm(\n           ^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n    return torch.batch_norm(\n           ^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 23.65 GiB total capacity; 21.57 GiB already allocated; 1001.31 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, ims \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m----> 7\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m   epoch_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28msum\u001b[39m(epoch_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(epoch_loss))\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(data, model, optimizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m ims_in, ims_out \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m pred_img \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims_in\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      6\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m myloss(pred_img\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), ims_out) \u001b[38;5;66;03m#custom loss function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_29007/3434515746.py\", line 102, in forward\n    x2 = self.down1(x1)\n         ^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_29007/3434515746.py\", line 29, in forward\n    return self.mpconv(x)\n           ^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_29007/3434515746.py\", line 18, in forward\n    return self.conv(x)\n           ^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 217, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n    return F.batch_norm(\n           ^^^^^^^^^^^^^\n  File \"/home/arshad/Downloads/venv/lib/python3.11/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n    return torch.batch_norm(\n           ^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 23.65 GiB total capacity; 21.57 GiB already allocated; 1001.31 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "for epoch in range(n_epoch):\n",
    "  print(epoch)\n",
    "  epoch_loss = []\n",
    "\n",
    "  for ix, ims in enumerate(train_loader):\n",
    "    loss = train_batch(ims, model, optimizer)\n",
    "    epoch_loss.append(loss)\n",
    "  print('avg_loss', sum(epoch_loss)/len(epoch_loss))\n",
    "  train_loss.append(sum(epoch_loss)/len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
